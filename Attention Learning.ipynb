{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "#conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention network, can be used on a sequence of any legnth of word embeddings\n",
    "class AttentionRnn(nn.Module):\n",
    "    def __init__(self, dim_input, dim_output):\n",
    "        super(AttentionRnn, self).__init__()\n",
    "        self.attention = nn.RNN(dim_input, dim_output, batch_first = True)\n",
    "        self.activation = nn.Sequential(nn.Sigmoid(),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        weights, hidden = self.attention(x)\n",
    "        weights = torch.squeeze(weights)\n",
    "        weights = self.activation(weights)\n",
    "        weighted_vector = torch.sum(x*weights[:,:,None],1)\n",
    "        return weighted_vector\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, k = 768):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.k = k\n",
    "        self.attention = AttentionRnn(768,1)\n",
    "        self.trading_strategy = nn.Sequential(\n",
    "            nn.Linear(self.k+5,1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        returns, topic_vectors = torch.tensor_split(x,[1], dim = 2) #split into word signals and price signals\n",
    "        weighted_vector = self.attention(topic_vectors)\n",
    "        #U,S,V = torch.pca_lowrank(weighted_vector,self.k)\n",
    "        #weighted_vector = torch.matmul(weighted_vector, V[:, :self.k])\n",
    "        returns = torch.squeeze(returns)\n",
    "        x = torch.cat((returns, weighted_vector), dim = 1)\n",
    "        return torch.squeeze(self.trading_strategy(x))\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(768+1,1, num_layers)\n",
    "        self.trading_strategy = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.lstm(x)\n",
    "        output = torch.squeeze(output)\n",
    "        output = self.trading_strategy(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    \n",
    "#model = NeuralNetwork()\n",
    "model = LSTM(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import btc price history\n",
    "btc_prices = pd.read_csv(\"btc_prices.csv\", index_col = 0)\n",
    "btc_prices.index = pd.to_datetime(btc_prices.index).strftime('%Y-%m-%d')\n",
    "btc_prices['log_ret'] = np.log(btc_prices.Close/btc_prices.Close.shift(1))\n",
    "btc_ret = btc_prices[[\"log_ret\"]].dropna()\n",
    "\n",
    "#get the right daterange\n",
    "btc_ret_train = btc_ret[(btc_ret.index >= '2016-01-01') & (btc_ret.index < '2021-01-01')].sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_ret</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>0.008711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>-0.002065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>-0.007938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>0.007137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>-0.002615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>-0.006251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>0.030458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>0.010198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>0.052625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>0.005559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1823 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             log_ret\n",
       "Date                \n",
       "2016-01-01  0.008711\n",
       "2016-01-02 -0.002065\n",
       "2016-01-03 -0.007938\n",
       "2016-01-04  0.007137\n",
       "2016-01-05 -0.002615\n",
       "...              ...\n",
       "2020-12-27 -0.006251\n",
       "2020-12-28  0.030458\n",
       "2020-12-29  0.010198\n",
       "2020-12-30  0.052625\n",
       "2020-12-31  0.005559\n",
       "\n",
       "[1823 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc_ret_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = pd.read_csv(\"bitcoin/btc_2016.csv\", index_col = 0)\n",
    "for i in [2017,2018,2019,2020]:\n",
    "    topic_vectors = topic_vectors.append(pd.read_csv(f\"bitcoin/btc_{i}.csv\", index_col = 0))\n",
    "topic_vectors.index = pd.date_range('2016-01-01', periods=len(topic_vectors))\n",
    "topic_vectors = btc_ret_train.join(topic_vectors, how= \"left\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the time series nicely streamlined into the machine learning models\n",
    "rolling_window = 5\n",
    "\n",
    "X_data = np.empty([len(topic_vectors)-rolling_window,rolling_window,769])\n",
    "y_series_data = np.empty([len(topic_vectors)-rolling_window,rolling_window])\n",
    "for i in range(0,len(topic_vectors)-rolling_window):\n",
    "    X_data[i] = np.array(topic_vectors.iloc[i:i+rolling_window,:])\n",
    "    y_series_data[i] = np.array(topic_vectors.iloc[i+1:i+1+rolling_window,0])\n",
    "y_data = np.array(topic_vectors.iloc[rolling_window:,0])\n",
    "\n",
    "X = X_data[:-200]\n",
    "X_tune = X_data[-200:-100]\n",
    "X_test = X_data[-100:]\n",
    "\n",
    "\n",
    "y = y_data[:-200]\n",
    "y_tune = y_data[-200:-100]\n",
    "y_test = y_data[-100:]\n",
    "\n",
    "y_series = y_series_data[:-200]\n",
    "y_series_tune = y_series_data[-200:-100]\n",
    "y_series_test = y_series_data[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (attention): AttentionRnn(\n",
      "    (attention): RNN(768, 1, batch_first=True)\n",
      "    (activation): Sequential(\n",
      "      (0): Sigmoid()\n",
      "      (1): Softmax(dim=1)\n",
      "    )\n",
      "  )\n",
      "  (trading_strategy): Linear(in_features=773, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_print = NeuralNetwork().to(device)\n",
    "print(model_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y_series).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "#test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.010295  [    0/ 1618]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.010066  [    0/ 1618]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.010838  [    0/ 1618]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.009193  [    0/ 1618]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.010151  [    0/ 1618]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.011321  [    0/ 1618]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.009777  [    0/ 1618]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.009915  [    0/ 1618]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.011708  [    0/ 1618]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.009976  [    0/ 1618]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.010426  [    0/ 1618]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.009681  [    0/ 1618]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.009803  [    0/ 1618]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.009865  [    0/ 1618]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.009583  [    0/ 1618]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.011060  [    0/ 1618]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.009685  [    0/ 1618]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.010057  [    0/ 1618]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.009752  [    0/ 1618]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.009992  [    0/ 1618]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.009885  [    0/ 1618]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.009935  [    0/ 1618]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.010050  [    0/ 1618]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.010217  [    0/ 1618]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.009821  [    0/ 1618]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.009673  [    0/ 1618]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.010119  [    0/ 1618]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.009435  [    0/ 1618]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.010868  [    0/ 1618]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.010285  [    0/ 1618]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.009467  [    0/ 1618]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.010452  [    0/ 1618]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.009772  [    0/ 1618]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.009874  [    0/ 1618]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.010725  [    0/ 1618]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.010302  [    0/ 1618]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.009749  [    0/ 1618]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.010516  [    0/ 1618]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.010575  [    0/ 1618]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.009860  [    0/ 1618]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.009768  [    0/ 1618]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.010203  [    0/ 1618]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.008933  [    0/ 1618]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.009133  [    0/ 1618]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.009806  [    0/ 1618]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.009295  [    0/ 1618]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.009050  [    0/ 1618]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.009024  [    0/ 1618]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.010120  [    0/ 1618]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.010165  [    0/ 1618]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.008589  [    0/ 1618]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.011443  [    0/ 1618]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.009441  [    0/ 1618]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.009312  [    0/ 1618]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.009812  [    0/ 1618]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.009376  [    0/ 1618]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.010715  [    0/ 1618]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.010072  [    0/ 1618]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.010376  [    0/ 1618]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.009607  [    0/ 1618]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.009286  [    0/ 1618]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.008894  [    0/ 1618]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.009895  [    0/ 1618]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.009606  [    0/ 1618]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.008965  [    0/ 1618]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.009427  [    0/ 1618]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.009280  [    0/ 1618]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.009620  [    0/ 1618]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.008580  [    0/ 1618]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.009903  [    0/ 1618]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.008391  [    0/ 1618]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.009339  [    0/ 1618]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.009480  [    0/ 1618]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.009054  [    0/ 1618]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.008799  [    0/ 1618]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.009280  [    0/ 1618]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.008912  [    0/ 1618]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.009059  [    0/ 1618]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.009269  [    0/ 1618]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.009324  [    0/ 1618]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.009414  [    0/ 1618]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.009589  [    0/ 1618]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.010129  [    0/ 1618]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.008890  [    0/ 1618]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.009798  [    0/ 1618]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.009401  [    0/ 1618]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.009775  [    0/ 1618]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.008796  [    0/ 1618]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.009142  [    0/ 1618]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.009340  [    0/ 1618]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.008830  [    0/ 1618]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.009686  [    0/ 1618]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.009496  [    0/ 1618]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.008893  [    0/ 1618]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.008998  [    0/ 1618]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.008749  [    0/ 1618]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.008956  [    0/ 1618]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.009724  [    0/ 1618]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.008899  [    0/ 1618]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.009823  [    0/ 1618]\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.008572  [    0/ 1618]\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.009436  [    0/ 1618]\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.008745  [    0/ 1618]\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.009198  [    0/ 1618]\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.008733  [    0/ 1618]\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.008521  [    0/ 1618]\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.008826  [    0/ 1618]\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.008379  [    0/ 1618]\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.008491  [    0/ 1618]\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.009238  [    0/ 1618]\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.009655  [    0/ 1618]\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.008506  [    0/ 1618]\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.010137  [    0/ 1618]\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.008709  [    0/ 1618]\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.009634  [    0/ 1618]\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.008855  [    0/ 1618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.009140  [    0/ 1618]\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.008991  [    0/ 1618]\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.008458  [    0/ 1618]\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.008300  [    0/ 1618]\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.010316  [    0/ 1618]\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.008458  [    0/ 1618]\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.008695  [    0/ 1618]\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.008091  [    0/ 1618]\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.008615  [    0/ 1618]\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.008997  [    0/ 1618]\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.010142  [    0/ 1618]\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.010806  [    0/ 1618]\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.008482  [    0/ 1618]\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.008106  [    0/ 1618]\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.009420  [    0/ 1618]\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.008076  [    0/ 1618]\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.008706  [    0/ 1618]\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.008313  [    0/ 1618]\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.008004  [    0/ 1618]\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.008468  [    0/ 1618]\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.008175  [    0/ 1618]\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.008244  [    0/ 1618]\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.008178  [    0/ 1618]\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.008751  [    0/ 1618]\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.008821  [    0/ 1618]\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.008159  [    0/ 1618]\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.007902  [    0/ 1618]\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.008517  [    0/ 1618]\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.008503  [    0/ 1618]\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.008548  [    0/ 1618]\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.009302  [    0/ 1618]\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.008438  [    0/ 1618]\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.008848  [    0/ 1618]\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.008357  [    0/ 1618]\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.008025  [    0/ 1618]\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.008648  [    0/ 1618]\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.007543  [    0/ 1618]\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.009189  [    0/ 1618]\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.008513  [    0/ 1618]\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.008041  [    0/ 1618]\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.009036  [    0/ 1618]\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.009574  [    0/ 1618]\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.008329  [    0/ 1618]\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.009331  [    0/ 1618]\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.007765  [    0/ 1618]\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.008372  [    0/ 1618]\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.007721  [    0/ 1618]\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.008857  [    0/ 1618]\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.007525  [    0/ 1618]\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.008443  [    0/ 1618]\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.008627  [    0/ 1618]\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.008134  [    0/ 1618]\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.009361  [    0/ 1618]\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.007737  [    0/ 1618]\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.007465  [    0/ 1618]\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.007837  [    0/ 1618]\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.008632  [    0/ 1618]\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.008724  [    0/ 1618]\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.007521  [    0/ 1618]\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.008454  [    0/ 1618]\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.008292  [    0/ 1618]\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.007880  [    0/ 1618]\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.008500  [    0/ 1618]\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.008385  [    0/ 1618]\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.008811  [    0/ 1618]\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.007613  [    0/ 1618]\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.007467  [    0/ 1618]\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.008094  [    0/ 1618]\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.007288  [    0/ 1618]\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.009449  [    0/ 1618]\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.007298  [    0/ 1618]\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.007693  [    0/ 1618]\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.007311  [    0/ 1618]\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.007713  [    0/ 1618]\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.007686  [    0/ 1618]\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.008719  [    0/ 1618]\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.007267  [    0/ 1618]\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.008762  [    0/ 1618]\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.007100  [    0/ 1618]\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.008814  [    0/ 1618]\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.007558  [    0/ 1618]\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.007774  [    0/ 1618]\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.009127  [    0/ 1618]\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.008281  [    0/ 1618]\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.007964  [    0/ 1618]\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.007739  [    0/ 1618]\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.008473  [    0/ 1618]\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.009832  [    0/ 1618]\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.007821  [    0/ 1618]\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.009088  [    0/ 1618]\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.007928  [    0/ 1618]\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.007719  [    0/ 1618]\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.008285  [    0/ 1618]\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.008267  [    0/ 1618]\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.007427  [    0/ 1618]\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.007438  [    0/ 1618]\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.007819  [    0/ 1618]\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.008739  [    0/ 1618]\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.008381  [    0/ 1618]\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.008272  [    0/ 1618]\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.007885  [    0/ 1618]\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.008000  [    0/ 1618]\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.007448  [    0/ 1618]\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.007461  [    0/ 1618]\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.007046  [    0/ 1618]\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.007313  [    0/ 1618]\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.007925  [    0/ 1618]\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.008556  [    0/ 1618]\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.008052  [    0/ 1618]\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.009020  [    0/ 1618]\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.006631  [    0/ 1618]\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.006988  [    0/ 1618]\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.007437  [    0/ 1618]\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.009591  [    0/ 1618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.007624  [    0/ 1618]\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.007378  [    0/ 1618]\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.008597  [    0/ 1618]\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.007109  [    0/ 1618]\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.008625  [    0/ 1618]\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.007857  [    0/ 1618]\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.007325  [    0/ 1618]\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.007841  [    0/ 1618]\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.007493  [    0/ 1618]\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.007380  [    0/ 1618]\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.008280  [    0/ 1618]\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.007148  [    0/ 1618]\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.006925  [    0/ 1618]\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.007454  [    0/ 1618]\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.007350  [    0/ 1618]\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.007871  [    0/ 1618]\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.006878  [    0/ 1618]\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.007032  [    0/ 1618]\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.008457  [    0/ 1618]\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.007863  [    0/ 1618]\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.007146  [    0/ 1618]\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.007291  [    0/ 1618]\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.006968  [    0/ 1618]\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.006983  [    0/ 1618]\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.006878  [    0/ 1618]\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.006578  [    0/ 1618]\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.007400  [    0/ 1618]\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.007739  [    0/ 1618]\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.008212  [    0/ 1618]\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.007443  [    0/ 1618]\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.007430  [    0/ 1618]\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.007435  [    0/ 1618]\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.007443  [    0/ 1618]\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.008884  [    0/ 1618]\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.007359  [    0/ 1618]\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.007042  [    0/ 1618]\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.007549  [    0/ 1618]\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.006877  [    0/ 1618]\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.006741  [    0/ 1618]\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.007029  [    0/ 1618]\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.006170  [    0/ 1618]\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.007164  [    0/ 1618]\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.008498  [    0/ 1618]\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.006759  [    0/ 1618]\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.006984  [    0/ 1618]\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.007556  [    0/ 1618]\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.006584  [    0/ 1618]\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.006649  [    0/ 1618]\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.007149  [    0/ 1618]\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.009681  [    0/ 1618]\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.007067  [    0/ 1618]\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.007224  [    0/ 1618]\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.007070  [    0/ 1618]\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.007831  [    0/ 1618]\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.006478  [    0/ 1618]\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.006553  [    0/ 1618]\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.006594  [    0/ 1618]\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.007025  [    0/ 1618]\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.006783  [    0/ 1618]\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.007554  [    0/ 1618]\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.007269  [    0/ 1618]\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.006493  [    0/ 1618]\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.006516  [    0/ 1618]\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.006856  [    0/ 1618]\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.007561  [    0/ 1618]\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.007815  [    0/ 1618]\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.007342  [    0/ 1618]\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.007203  [    0/ 1618]\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.006563  [    0/ 1618]\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.006515  [    0/ 1618]\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.006667  [    0/ 1618]\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.008101  [    0/ 1618]\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.006578  [    0/ 1618]\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.007525  [    0/ 1618]\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.006613  [    0/ 1618]\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.007727  [    0/ 1618]\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.007011  [    0/ 1618]\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.007298  [    0/ 1618]\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.006516  [    0/ 1618]\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.006988  [    0/ 1618]\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.007897  [    0/ 1618]\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.007547  [    0/ 1618]\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.006513  [    0/ 1618]\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.008173  [    0/ 1618]\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.006540  [    0/ 1618]\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.007469  [    0/ 1618]\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.006570  [    0/ 1618]\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.006701  [    0/ 1618]\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.006861  [    0/ 1618]\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.005804  [    0/ 1618]\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.006392  [    0/ 1618]\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.006462  [    0/ 1618]\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.006921  [    0/ 1618]\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.006712  [    0/ 1618]\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.006312  [    0/ 1618]\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.007407  [    0/ 1618]\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.006533  [    0/ 1618]\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.006620  [    0/ 1618]\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.007854  [    0/ 1618]\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.006010  [    0/ 1618]\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.006386  [    0/ 1618]\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.006524  [    0/ 1618]\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.007051  [    0/ 1618]\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.005711  [    0/ 1618]\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.005689  [    0/ 1618]\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.006298  [    0/ 1618]\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.006802  [    0/ 1618]\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.006242  [    0/ 1618]\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.006151  [    0/ 1618]\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.006862  [    0/ 1618]\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.006235  [    0/ 1618]\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.006929  [    0/ 1618]\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.007139  [    0/ 1618]\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.006890  [    0/ 1618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.006681  [    0/ 1618]\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.005749  [    0/ 1618]\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.006176  [    0/ 1618]\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.006441  [    0/ 1618]\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.006649  [    0/ 1618]\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.006359  [    0/ 1618]\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.007720  [    0/ 1618]\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.005965  [    0/ 1618]\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.005767  [    0/ 1618]\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.007009  [    0/ 1618]\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.006513  [    0/ 1618]\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.006005  [    0/ 1618]\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.006435  [    0/ 1618]\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.007027  [    0/ 1618]\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.006412  [    0/ 1618]\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.005862  [    0/ 1618]\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.006173  [    0/ 1618]\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.006920  [    0/ 1618]\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.006353  [    0/ 1618]\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.006192  [    0/ 1618]\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.005615  [    0/ 1618]\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.006764  [    0/ 1618]\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.006463  [    0/ 1618]\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.006568  [    0/ 1618]\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.006440  [    0/ 1618]\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.008523  [    0/ 1618]\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.007211  [    0/ 1618]\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.006441  [    0/ 1618]\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.006141  [    0/ 1618]\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.006703  [    0/ 1618]\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.006163  [    0/ 1618]\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.006533  [    0/ 1618]\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.005813  [    0/ 1618]\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.007800  [    0/ 1618]\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.006336  [    0/ 1618]\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.005958  [    0/ 1618]\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.005761  [    0/ 1618]\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.006352  [    0/ 1618]\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.006277  [    0/ 1618]\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.007077  [    0/ 1618]\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.007211  [    0/ 1618]\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.006257  [    0/ 1618]\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.006754  [    0/ 1618]\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.006821  [    0/ 1618]\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.006569  [    0/ 1618]\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.005821  [    0/ 1618]\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.005895  [    0/ 1618]\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.005759  [    0/ 1618]\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.007441  [    0/ 1618]\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.006354  [    0/ 1618]\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.006446  [    0/ 1618]\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.006713  [    0/ 1618]\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.006123  [    0/ 1618]\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.005759  [    0/ 1618]\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.006395  [    0/ 1618]\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.006261  [    0/ 1618]\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.007244  [    0/ 1618]\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.005808  [    0/ 1618]\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.005597  [    0/ 1618]\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.005468  [    0/ 1618]\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.006000  [    0/ 1618]\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.005576  [    0/ 1618]\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.006214  [    0/ 1618]\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.006154  [    0/ 1618]\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.005697  [    0/ 1618]\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.005751  [    0/ 1618]\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.006225  [    0/ 1618]\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.005788  [    0/ 1618]\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.006300  [    0/ 1618]\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.005859  [    0/ 1618]\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.006069  [    0/ 1618]\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.007025  [    0/ 1618]\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.005537  [    0/ 1618]\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.006845  [    0/ 1618]\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.006418  [    0/ 1618]\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.006661  [    0/ 1618]\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.006167  [    0/ 1618]\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.005753  [    0/ 1618]\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.006425  [    0/ 1618]\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.005680  [    0/ 1618]\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.006407  [    0/ 1618]\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.006703  [    0/ 1618]\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.005730  [    0/ 1618]\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.005665  [    0/ 1618]\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.006902  [    0/ 1618]\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.005506  [    0/ 1618]\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.006464  [    0/ 1618]\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.006578  [    0/ 1618]\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.006602  [    0/ 1618]\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.006577  [    0/ 1618]\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.007177  [    0/ 1618]\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.006724  [    0/ 1618]\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.005840  [    0/ 1618]\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.006448  [    0/ 1618]\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.007256  [    0/ 1618]\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.005772  [    0/ 1618]\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.006018  [    0/ 1618]\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.005656  [    0/ 1618]\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.006587  [    0/ 1618]\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.006246  [    0/ 1618]\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.006020  [    0/ 1618]\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.006357  [    0/ 1618]\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.005507  [    0/ 1618]\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.005988  [    0/ 1618]\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.005902  [    0/ 1618]\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.005615  [    0/ 1618]\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.006354  [    0/ 1618]\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.006600  [    0/ 1618]\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.005875  [    0/ 1618]\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.005723  [    0/ 1618]\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.006039  [    0/ 1618]\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.006170  [    0/ 1618]\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.005351  [    0/ 1618]\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.005525  [    0/ 1618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.005979  [    0/ 1618]\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.005714  [    0/ 1618]\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.005317  [    0/ 1618]\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.005806  [    0/ 1618]\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.005271  [    0/ 1618]\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.006763  [    0/ 1618]\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.005522  [    0/ 1618]\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.005209  [    0/ 1618]\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.006293  [    0/ 1618]\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.006560  [    0/ 1618]\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.005873  [    0/ 1618]\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.005565  [    0/ 1618]\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.005161  [    0/ 1618]\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.006933  [    0/ 1618]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8bb60a49a5b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m#test_loop(test_dataloader, model, loss_fn)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e2db00cee1d4>\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 500\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    #test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true, pred):\n",
    "    temp = np.sign(true*pred)+1\n",
    "    print(temp)\n",
    "    return sum(temp)/len(temp)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.tensor(X).float()\n",
    "y_value = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.tensor(X_test).float()\n",
    "y_value = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1618,) (1618,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0096db06dfa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-9a5bc06615ee>\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(true, pred)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1618,) (1618,5) "
     ]
    }
   ],
   "source": [
    "pred =model(x_test).detach().numpy()\n",
    "accuracy(y_value,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 0. 0. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 2. 2. 0. 2. 0. 2. 2. 0. 2. 2. 2. 0. 0. 2. 2. 2. 0. 0. 2. 0.\n",
      " 0. 2. 2. 2. 0. 0. 2. 2. 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 2. 2. 0. 2. 2.\n",
      " 0. 2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 2. 0. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.squeeze(torch.tensor_split(model(x_test),[4], dim = 1)[1]).detach().numpy()\n",
    "accuracy(y_value,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"LSTM_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"LSTM_weights.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
